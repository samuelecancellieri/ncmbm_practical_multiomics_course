{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b1cdba14",
      "metadata": {},
      "source": [
        "# ChIP-seq Processing Pipeline - Student Exercise\n",
        "\n",
        "## Learning Objectives\n",
        "By completing this notebook, you will:\n",
        "- Understand the ChIP-seq data analysis workflow\n",
        "- Learn how to process single-end sequencing reads\n",
        "- Identify protein-DNA binding sites\n",
        "- Assess data quality using standard metrics\n",
        "\n",
        "## Pipeline Overview\n",
        "```\n",
        "Raw FASTQ ‚Üí Trimming ‚Üí Alignment ‚Üí Mark Duplicates ‚Üí Filter ‚Üí Coverage Track\n",
        "   (Input)  (Trim Galore) (Bowtie2)    (Picard)      (QC)    (BigWig)\n",
        "```\n",
        "\n",
        "## What is ChIP-seq?\n",
        "**ChIP-seq** (Chromatin Immunoprecipitation sequencing) identifies genome-wide DNA binding sites for transcription factors and other proteins.\n",
        "\n",
        "**Key Concepts:**\n",
        "- **IP (Immunoprecipitation) sample**: DNA fragments bound to your protein of interest\n",
        "- **Input/Control sample**: Background DNA (no immunoprecipitation)\n",
        "- **Peak calling**: Identifies regions enriched in IP vs. control\n",
        "\n",
        "## Pipeline Steps\n",
        "This notebook covers **sample processing** (both IP and control samples):\n",
        "1. Quality control and trimming\n",
        "2. Read alignment\n",
        "3. Duplicate marking\n",
        "4. Quality filtering\n",
        "5. Coverage track generation\n",
        "\n",
        "**Note**: Peak calling is done separately using both IP and control samples together.\n",
        "\n",
        "## Required Tools\n",
        "- **Trim Galore** (0.6.10): Adapter trimming and QC\n",
        "- **Bowtie2** (2.4.1): Read aligner\n",
        "- **Samtools** (1.7): BAM file processing\n",
        "- **Picard** (2.23.4): Duplicate marking\n",
        "- **deeptools** (3.5.5): Coverage visualization\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions**: Follow the cells below and complete sections marked with `# TODO`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5f2aa90",
      "metadata": {},
      "source": [
        "## Step 1: Import Python Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc301155",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚úì Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1988b4",
      "metadata": {},
      "source": [
        "## Step 2: Define Bioinformatics Tool Containers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee3b2ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Container images for each tool\n",
        "trimgalore_container = \"quay.io/biocontainers/trim-galore:0.6.10--hdfd78af_1\"\n",
        "fastqc_container = \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n",
        "bowtie2_container = \"quay.io/biocontainers/bowtie2:2.4.1--py38h1c8e9b9_3\"\n",
        "samtools_container = \"quay.io/biocontainers/samtools:1.7--2\"\n",
        "picard_container = \"quay.io/biocontainers/picard:2.23.4--0\"\n",
        "deeptools_container = \"quay.io/biocontainers/deeptools:3.5.5--pyhdfd78af_0\"\n",
        "\n",
        "print(\"‚úì Container images defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f7fd7d",
      "metadata": {},
      "source": [
        "## Step 3: Set Your Pipeline Parameters\n",
        "\n",
        "**üìù TODO: Update these paths with your actual data!**\n",
        "\n",
        "**Important**: Run this pipeline twice - once for your IP sample and once for your control/input sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cfcdfd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Update these paths to match your data location\n",
        "fastq1 = \"/path/to/your/sample_R1.fq.gz\"      # Single-end reads\n",
        "basename = \"my_chip_sample\"                    # Sample name (e.g., \"IP_sample\" or \"control\")\n",
        "output_dir = \"/path/to/output_directory\"       # Where to save results\n",
        "\n",
        "# Reference genome files (ask your instructor for these paths)\n",
        "genome_index = \"/path/to/bowtie2_index\"        # Bowtie2 genome index\n",
        "\n",
        "# Analysis settings\n",
        "threads = 2                                     # Number of CPU threads\n",
        "# TODO: Update this to your accessible directory for Singularity\n",
        "BIND_DIR = \"/path/to/accessible_directory/\"    # Directory accessible to Singularity\n",
        "\n",
        "print(f\"Sample name: {basename}\")\n",
        "print(f\"Sample type: {fastq1}\")\n",
        "print(f\"Threads: {threads}\")\n",
        "print(\"‚ö† Remember to update the file paths above before running!\")\n",
        "print(\"\\nüí° Tip: Process both IP and control samples using this notebook!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee3e62b",
      "metadata": {},
      "source": [
        "## Step 4: Create Output Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d2ad43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory structure\n",
        "OUTPUT_DIR = os.path.join(os.path.abspath(output_dir), f\"{basename}_results\")\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create subdirectories\n",
        "qc_dir = os.path.join(OUTPUT_DIR, f\"{basename}_qc\")\n",
        "picard_dir = os.path.join(OUTPUT_DIR, f\"{basename}_picard\")\n",
        "\n",
        "for d in [qc_dir, picard_dir]:\n",
        "    Path(d).mkdir(exist_ok=True)\n",
        "\n",
        "# Initialize log file\n",
        "log_file = os.path.join(OUTPUT_DIR, f\"{basename}_pipeline.log\")\n",
        "\n",
        "print(f\"‚úì Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"‚úì QC directory: {qc_dir}\")\n",
        "print(f\"‚úì Picard directory: {picard_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7880052",
      "metadata": {},
      "source": [
        "## Step 5: Trim Adapters and Run Quality Control\n",
        "\n",
        "**What does this do?** Removes adapter sequences and low-quality bases, then runs FastQC on trimmed reads.\n",
        "\n",
        "**üìù TODO: Run Trim Galore with integrated FastQC.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bcc3567",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: Trimming and Quality Control\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nRunning Trim Galore (with FastQC)...\")\n",
        "\n",
        "# TODO: Complete the Trim Galore command\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{trimgalore_container}\",  # TODO: Which container?\n",
        "    \"trim_galore\",\n",
        "    \"-j\", str(threads),\n",
        "    \"--gzip\",\n",
        "    \"--basename\", basename,\n",
        "    \"--fastqc_args\", f\"--outdir {qc_dir}\",\n",
        "    \"-o\", OUTPUT_DIR,\n",
        "    fastq1\n",
        "]\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Trimming and QC completed!\")\n",
        "    fastq1_trimmed = os.path.join(OUTPUT_DIR, f\"{basename}_trimmed.fq.gz\")\n",
        "    print(f\"  Trimmed reads: {fastq1_trimmed}\")\n",
        "    print(f\"  QC reports: {qc_dir}/\")\n",
        "    \n",
        "    # Show trimming stats from output\n",
        "    if \"Total reads processed:\" in result.stdout:\n",
        "        for line in result.stdout.split('\\n'):\n",
        "            if 'reads processed' in line or 'reads with adapters' in line or 'Quality-trimmed' in line:\n",
        "                print(f\"  {line.strip()}\")\n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")\n",
        "    \n",
        "# Question: What percentage of your reads contained adapters?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8b440d",
      "metadata": {},
      "source": [
        "## Step 6: Align Reads with Bowtie2\n",
        "\n",
        "**What does this do?** Maps single-end reads to the reference genome.\n",
        "\n",
        "**üìù TODO: Complete the Bowtie2 alignment command.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b283423",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 2: Alignment with Bowtie2\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fastq1_trimmed = os.path.join(OUTPUT_DIR, f\"{basename}_trimmed.fq.gz\")\n",
        "sam_file = os.path.join(OUTPUT_DIR, f\"{basename}.sam\")\n",
        "\n",
        "print(\"\\nAligning reads to genome...\")\n",
        "\n",
        "# TODO: Complete the Bowtie2 command for single-end reads\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{bowtie2_container}\",  # TODO: Which container?\n",
        "    \"bowtie2\",\n",
        "    \"--very-sensitive\",     # Sensitive alignment mode\n",
        "    \"-p\", str(threads),\n",
        "    \"-x\", genome_index,\n",
        "    \"-U\", fastq1_trimmed,   # -U for single-end (vs -1/-2 for paired)\n",
        "]\n",
        "\n",
        "print(\"Running Bowtie2 (this may take several minutes)...\")\n",
        "with open(sam_file, 'w') as outfile:\n",
        "    result = subprocess.run(cmd, stdout=outfile, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Alignment completed!\")\n",
        "    print(f\"  SAM file: {sam_file}\")\n",
        "    \n",
        "    # Parse alignment stats\n",
        "    print(\"\\n  Alignment Statistics:\")\n",
        "    for line in result.stderr.split('\\n'):\n",
        "        if 'reads; of these:' in line or 'aligned concordantly' in line or 'aligned exactly' in line or 'overall alignment rate' in line:\n",
        "            print(f\"  {line.strip()}\")\n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")\n",
        "    \n",
        "# Question: What is your overall alignment rate? Is it acceptable (>70%)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d63e638",
      "metadata": {},
      "source": [
        "## Step 7: Convert SAM to BAM and Sort\n",
        "\n",
        "**What does this do?** Converts to binary format (BAM) and sorts by genomic coordinates.\n",
        "\n",
        "**üìù TODO: Complete the conversion and sorting steps.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9ca290",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 3: Convert SAM to BAM and Sort\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sam_file = os.path.join(OUTPUT_DIR, f\"{basename}.sam\")\n",
        "sorted_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted.bam\")\n",
        "\n",
        "print(\"\\n[1/2] Converting SAM to BAM and sorting...\")\n",
        "\n",
        "# TODO: Complete the samtools sort command\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{samtools_container}\",  # TODO: Which container?\n",
        "    \"samtools\", \"sort\",\n",
        "    \"-@\", str(threads),\n",
        "    \"-o\", sorted_bam,\n",
        "    sam_file\n",
        "]\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Sorting completed!\")\n",
        "    print(f\"  Sorted BAM: {sorted_bam}\")\n",
        "    \n",
        "    # Get file size\n",
        "    bam_size = os.path.getsize(sorted_bam) / (1024**3)  # Convert to GB\n",
        "    print(f\"  File size: {bam_size:.2f} GB\")\n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")\n",
        "\n",
        "print(\"\\n[2/2] Fixing BAM header (removing Bowtie2 version info)...\")\n",
        "# This step is needed for compatibility with Picard\n",
        "header_file = os.path.join(OUTPUT_DIR, f\"{basename}_header.txt\")\n",
        "header_filtered = os.path.join(OUTPUT_DIR, f\"{basename}_header_filtered.txt\")\n",
        "\n",
        "# Extract header\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{samtools_container}\",\n",
        "    \"samtools\", \"view\", \"-H\", sorted_bam\n",
        "]\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "with open(header_file, 'w') as f:\n",
        "    f.write(result.stdout)\n",
        "\n",
        "# Filter @PG lines\n",
        "with open(header_file, 'r') as f:\n",
        "    with open(header_filtered, 'w') as out:\n",
        "        for line in f:\n",
        "            if not line.startswith('@PG'):\n",
        "                out.write(line)\n",
        "\n",
        "# Reheader\n",
        "reheaded_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted_reheaded.bam\")\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{samtools_container}\",\n",
        "    \"samtools\", \"reheader\",\n",
        "    \"-P\", header_filtered,\n",
        "    sorted_bam\n",
        "]\n",
        "with open(reheaded_bam, 'w') as outfile:\n",
        "    subprocess.run(cmd, stdout=outfile, capture_output=False)\n",
        "\n",
        "# Replace original\n",
        "os.replace(reheaded_bam, sorted_bam)\n",
        "os.remove(header_file)\n",
        "os.remove(header_filtered)\n",
        "os.remove(sam_file)\n",
        "\n",
        "print(\"‚úì Header fixed and SAM file removed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba01e679",
      "metadata": {},
      "source": [
        "## Step 8: Mark Duplicates with Picard\n",
        "\n",
        "**What does this do?** Identifies PCR duplicate reads (same genomic position).\n",
        "\n",
        "**Why mark duplicates in ChIP-seq?** PCR duplicates can artificially inflate signal and should be marked (and often removed) to avoid false positive peaks.\n",
        "\n",
        "**üìù TODO: Run Picard MarkDuplicates.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a9b5a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 4: Mark duplicate reads\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sorted_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted.bam\")\n",
        "mkdup_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted_mkdup.bam\")\n",
        "dup_metrics = os.path.join(picard_dir, f\"{basename}_dup.txt\")\n",
        "\n",
        "print(\"\\nMarking duplicates with Picard...\")\n",
        "\n",
        "# TODO: Complete the Picard command\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{picard_container}\",  # TODO: Which container?\n",
        "    \"picard\", \"MarkDuplicates\",\n",
        "    \"-I\", sorted_bam,\n",
        "    \"-O\", mkdup_bam,\n",
        "    \"-M\", dup_metrics,\n",
        "    \"-REMOVE_DUPLICATES\", \"false\",  # Mark but don't remove yet\n",
        "    \"-ASSUME_SORT_ORDER\", \"coordinate\"\n",
        "]\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Duplicate marking completed!\")\n",
        "    print(f\"  Marked BAM: {mkdup_bam}\")\n",
        "    print(f\"  Metrics file: {dup_metrics}\")\n",
        "    \n",
        "    # Parse duplicate metrics\n",
        "    print(\"\\n  üìä Duplicate Statistics:\")\n",
        "    with open(dup_metrics, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"LIBRARY\"):\n",
        "                header = line.strip().split('\\t')\n",
        "                data = next(f).strip().split('\\t')\n",
        "                # Find indices\n",
        "                for i, col in enumerate(header):\n",
        "                    if col == \"UNPAIRED_READS_EXAMINED\":\n",
        "                        unpaired_reads = int(data[i])\n",
        "                    elif col == \"UNPAIRED_READ_DUPLICATES\":\n",
        "                        unpaired_dups = int(data[i])\n",
        "                    elif col == \"PERCENT_DUPLICATION\":\n",
        "                        dup_rate = float(data[i])\n",
        "                \n",
        "                print(f\"    Total reads examined: {unpaired_reads:,}\")\n",
        "                print(f\"    Duplicate reads: {unpaired_dups:,}\")\n",
        "                print(f\"    Duplication rate: {dup_rate*100:.2f}%\")\n",
        "                \n",
        "                if dup_rate < 0.2:\n",
        "                    print(f\"    ‚úì Good library complexity (<20% duplicates)\")\n",
        "                elif dup_rate < 0.5:\n",
        "                    print(f\"    ‚ö† Moderate duplication (20-50%)\")\n",
        "                else:\n",
        "                    print(f\"    ‚úó High duplication (>50%) - may indicate low complexity\")\n",
        "                break\n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")\n",
        "    \n",
        "# Question: What is your duplication rate? What does it tell you about library quality?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a546c96f",
      "metadata": {},
      "source": [
        "## Step 9: Filter Low Quality Reads and Remove Duplicates\n",
        "\n",
        "**What does this do?** Removes:\n",
        "- Duplicate reads (FLAG 1024)\n",
        "- Low mapping quality reads (MAPQ < 20)\n",
        "\n",
        "**MAPQ threshold**: 20 means 99% probability the read is correctly mapped.\n",
        "\n",
        "**üìù TODO: Apply quality filters.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05892739",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 5: Filter low quality reads and remove duplicates\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "mkdup_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted_mkdup.bam\")\n",
        "filtered_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted_mkdup_filtered.bam\")\n",
        "final_bam = os.path.join(OUTPUT_DIR, f\"{basename}_final.bam\")\n",
        "\n",
        "print(\"\\nFiltering reads with MAPQ < 20 and removing duplicates...\")\n",
        "\n",
        "# TODO: Complete the filtering command\n",
        "# Hint: -F 1024 removes duplicates, -q 20 keeps only high-quality alignments\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{samtools_container}\",\n",
        "    \"samtools\", \"view\",\n",
        "    \"-@\", str(threads),\n",
        "    \"-F\", \"1024\",     # Remove duplicate flag\n",
        "    \"-q\", \"20\",       # Minimum MAPQ = 20\n",
        "    \"-b\",             # Output BAM\n",
        "    \"-o\", filtered_bam,\n",
        "    mkdup_bam\n",
        "]\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Filtering completed!\")\n",
        "    \n",
        "    # Sort the filtered BAM\n",
        "    print(\"Sorting filtered BAM...\")\n",
        "    cmd = [\n",
        "        \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "        \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "        f\"docker://{samtools_container}\",\n",
        "        \"samtools\", \"sort\",\n",
        "        \"-@\", str(threads),\n",
        "        \"-o\", final_bam,\n",
        "        filtered_bam\n",
        "    ]\n",
        "    subprocess.run(cmd, capture_output=True)\n",
        "    \n",
        "    print(\"‚úì Final BAM created!\")\n",
        "    print(f\"  Final BAM: {final_bam}\")\n",
        "    \n",
        "    # Generate flagstats\n",
        "    flagstats_file = os.path.join(qc_dir, f\"{basename}_flagstats.txt\")\n",
        "    cmd = [\n",
        "        \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "        \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "        f\"docker://{samtools_container}\",\n",
        "        \"samtools\", \"flagstat\",\n",
        "        \"-@\", str(threads),\n",
        "        final_bam\n",
        "    ]\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    \n",
        "    with open(flagstats_file, 'w') as f:\n",
        "        f.write(result.stdout)\n",
        "    \n",
        "    print(f\"\\n  üìä Final Read Statistics:\")\n",
        "    for line in result.stdout.split('\\n'):\n",
        "        if 'mapped (' in line or 'properly paired' in line or 'singletons' in line:\n",
        "            print(f\"    {line.strip()}\")\n",
        "    \n",
        "    print(f\"\\n  Full stats saved to: {flagstats_file}\")\n",
        "    \n",
        "    # Clean up intermediate files\n",
        "    os.remove(mkdup_bam)\n",
        "    os.remove(filtered_bam)\n",
        "    print(\"\\n‚úì Intermediate files cleaned up\")\n",
        "    \n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88e66493",
      "metadata": {},
      "source": [
        "## Step 10: Create BigWig Coverage Track\n",
        "\n",
        "**What does this do?** Generates a normalized genome browser track for visualization.\n",
        "\n",
        "**Normalization**: RPKM (Reads Per Kilobase per Million mapped reads) makes samples comparable.\n",
        "\n",
        "**üìù TODO: Generate the coverage track.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323ae6b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 6: Generate BigWig coverage track\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "final_bam = os.path.join(OUTPUT_DIR, f\"{basename}_final.bam\")\n",
        "bigwig_file = os.path.join(OUTPUT_DIR, f\"{basename}_final_RPKM_Norm_bs10.bw\")\n",
        "\n",
        "print(\"\\n[1/2] Indexing BAM file...\")\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{samtools_container}\",\n",
        "    \"samtools\", \"index\",\n",
        "    \"-@\", str(threads),\n",
        "    final_bam\n",
        "]\n",
        "subprocess.run(cmd, capture_output=True)\n",
        "print(\"‚úì Indexing done\")\n",
        "\n",
        "print(\"\\n[2/2] Creating normalized BigWig track...\")\n",
        "\n",
        "# TODO: Complete the bamCoverage command\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{deeptools_container}\",  # TODO: Which container?\n",
        "    \"bamCoverage\",\n",
        "    \"-b\", final_bam,\n",
        "    \"-o\", bigwig_file,\n",
        "    \"-bs\", \"10\",                # 10bp bins\n",
        "    \"--normalizeUsing\", \"RPKM\", # RPKM normalization\n",
        "    \"-p\", str(threads)\n",
        "]\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì BigWig file created!\")\n",
        "    print(f\"  File: {bigwig_file}\")\n",
        "    \n",
        "    # Get file size\n",
        "    bw_size = os.path.getsize(bigwig_file) / (1024**2)  # Convert to MB\n",
        "    print(f\"  File size: {bw_size:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\n  üí° Visualization:\")\n",
        "    print(f\"     - Load into IGV (Integrative Genomics Viewer)\")\n",
        "    print(f\"     - Load into UCSC Genome Browser\")\n",
        "    print(f\"     - Compare IP vs Control tracks to see enrichment\")\n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "046a6cbd",
      "metadata": {},
      "source": [
        "## Step 11: Pipeline Summary\n",
        "\n",
        "**Congratulations!** üéâ You've successfully processed your ChIP-seq sample!\n",
        "\n",
        "Let's review the results and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4da544d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"CHIP-SEQ SAMPLE PROCESSING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\n‚úì Sample processed: {basename}\")\n",
        "print(f\"‚úì Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"QUALITY METRICS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Read flagstats\n",
        "flagstats_file = os.path.join(qc_dir, f\"{basename}_flagstats.txt\")\n",
        "if os.path.exists(flagstats_file):\n",
        "    print(\"\\n1. Alignment Statistics:\")\n",
        "    with open(flagstats_file, 'r') as f:\n",
        "        content = f.read()\n",
        "        print(f\"   {content}\")\n",
        "\n",
        "# Read duplicate metrics\n",
        "dup_metrics = os.path.join(picard_dir, f\"{basename}_dup.txt\")\n",
        "if os.path.exists(dup_metrics):\n",
        "    print(\"\\n2. Duplication Rate:\")\n",
        "    with open(dup_metrics, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"LIBRARY\"):\n",
        "                header = line.strip().split('\\t')\n",
        "                data = next(f).strip().split('\\t')\n",
        "                for i, col in enumerate(header):\n",
        "                    if col == \"PERCENT_DUPLICATION\":\n",
        "                        dup_rate = float(data[i])\n",
        "                        print(f\"   {dup_rate*100:.2f}%\")\n",
        "                        if dup_rate < 0.2:\n",
        "                            print(f\"   ‚úì Good library quality\")\n",
        "                        elif dup_rate < 0.5:\n",
        "                            print(f\"   ‚ö† Moderate duplication\")\n",
        "                        else:\n",
        "                            print(f\"   ‚úó High duplication\")\n",
        "                break\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"OUTPUT FILES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìÅ Results Directory: {OUTPUT_DIR}\")\n",
        "print(f\"\\n  Key Files:\")\n",
        "print(f\"  ‚îú‚îÄ {basename}_final.bam              ‚Üê Final aligned reads (use for peak calling)\")\n",
        "print(f\"  ‚îú‚îÄ {basename}_final_RPKM_Norm_bs10.bw ‚Üê Coverage track (load in IGV)\")\n",
        "print(f\"  ‚îú‚îÄ {basename}_qc/\")\n",
        "print(f\"  ‚îÇ  ‚îú‚îÄ {basename}_flagstats.txt        ‚Üê Alignment statistics\")\n",
        "print(f\"  ‚îÇ  ‚îî‚îÄ FastQC reports                  ‚Üê Quality control reports\")\n",
        "print(f\"  ‚îî‚îÄ {basename}_picard/\")\n",
        "print(f\"     ‚îî‚îÄ {basename}_dup.txt              ‚Üê Duplicate metrics\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"NEXT STEPS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: Process BOTH samples!\")\n",
        "print(\"   1. Run this notebook for your IP sample\")\n",
        "print(\"   2. Run this notebook again for your control/input sample\")\n",
        "\n",
        "print(\"\\nüìä After processing both samples:\")\n",
        "print(\"   Use a separate peak calling script/notebook with:\")\n",
        "print(f\"   - IP BAM file: {basename}_final.bam\")\n",
        "print(\"   - Control BAM file: [control_name]_final.bam\")\n",
        "\n",
        "print(\"\\nüîç Peak Calling Tools:\")\n",
        "print(\"   - MACS2 (most common, good for transcription factors)\")\n",
        "print(\"   - MACS3 (updated version)\")\n",
        "print(\"   - HOMER (good for histone marks)\")\n",
        "print(\"   - SICER (good for broad domains)\")\n",
        "\n",
        "print(\"\\nüìà Downstream Analysis:\")\n",
        "print(\"   1. Peak annotation (assign peaks to genes)\")\n",
        "print(\"   2. Motif analysis (find DNA binding motifs)\")\n",
        "print(\"   3. Differential binding analysis (compare conditions)\")\n",
        "print(\"   4. Gene ontology enrichment\")\n",
        "print(\"   5. Integration with RNA-seq data\")\n",
        "\n",
        "print(\"\\nüí° Visualization Tips:\")\n",
        "print(\"   - Load BigWig files for both IP and control into IGV\")\n",
        "print(\"   - Look for regions where IP signal > control signal\")\n",
        "print(\"   - These regions indicate protein binding sites\")\n",
        "\n",
        "print(\"\\nüìù Review Questions:\")\n",
        "print(\"   1. What is your alignment rate? Is it acceptable?\")\n",
        "print(\"   2. What is your duplication rate? What does it mean?\")\n",
        "print(\"   3. How many reads remained after filtering?\")\n",
        "print(\"   4. What is the difference between IP and control samples?\")\n",
        "print(\"   5. Why do we need both IP and control for peak calling?\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Great job! Your ChIP-seq sample is ready for peak calling! üéâ\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
