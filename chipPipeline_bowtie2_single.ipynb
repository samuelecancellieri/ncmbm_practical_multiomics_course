{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b1cdba14",
      "metadata": {},
      "source": [
        "# ChIP-seq Processing Pipeline - Student Exercise\n",
        "\n",
        "## Learning Objectives\n",
        "By completing this notebook, you will:\n",
        "- Understand the ChIP-seq data analysis workflow\n",
        "- Learn how to process single-end sequencing reads\n",
        "- Identify protein-DNA binding sites\n",
        "- Assess data quality using standard metrics\n",
        "\n",
        "## Pipeline Overview\n",
        "```\n",
        "Raw FASTQ ‚Üí Trimming ‚Üí Alignment ‚Üí Mark Duplicates ‚Üí Filter ‚Üí Coverage Track\n",
        "   (Input)  (Trim Galore) (Bowtie2)    (Picard)      (QC)    (BigWig)\n",
        "```\n",
        "\n",
        "## What is ChIP-seq?\n",
        "**ChIP-seq** (Chromatin Immunoprecipitation sequencing) identifies genome-wide DNA binding sites for transcription factors and other proteins.\n",
        "\n",
        "**Key Concepts:**\n",
        "- **IP (Immunoprecipitation) sample**: DNA fragments bound to your protein of interest\n",
        "- **Input/Control sample**: Background DNA (no immunoprecipitation)\n",
        "- **Peak calling**: Identifies regions enriched in IP vs. control\n",
        "\n",
        "## Pipeline Steps\n",
        "This notebook covers **sample processing** (both IP and control samples):\n",
        "1. Quality control and trimming\n",
        "2. Read alignment\n",
        "3. Duplicate marking\n",
        "4. Quality filtering\n",
        "5. Coverage track generation\n",
        "\n",
        "**Note**: Peak calling is done separately using both IP and control samples together.\n",
        "\n",
        "## Required Tools\n",
        "- **Trim Galore** (0.6.10): Adapter trimming and QC\n",
        "- **Bowtie2** (2.4.1): Read aligner\n",
        "- **Samtools** (1.7): BAM file processing\n",
        "- **Picard** (2.23.4): Duplicate marking\n",
        "- **deeptools** (3.5.5): Coverage visualization\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions**: Follow the cells below and complete sections marked with `# TODO`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5f2aa90",
      "metadata": {},
      "source": [
        "## Step 1: Import Python Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc301155",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚úì Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1988b4",
      "metadata": {},
      "source": [
        "## Step 2: Define Bioinformatics Tool Containers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee3b2ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Container images for each tool\n",
        "trimgalore_container = \"quay.io/biocontainers/trim-galore:0.6.10--hdfd78af_1\"\n",
        "fastqc_container = \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n",
        "bowtie2_container = \"quay.io/biocontainers/bowtie2:2.4.1--py38h1c8e9b9_3\"\n",
        "samtools_container = \"quay.io/biocontainers/samtools:1.7--2\"\n",
        "picard_container = \"quay.io/biocontainers/picard:2.23.4--0\"\n",
        "deeptools_container = \"quay.io/biocontainers/deeptools:3.5.5--pyhdfd78af_0\"\n",
        "\n",
        "print(\"‚úì Container images defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f7fd7d",
      "metadata": {},
      "source": [
        "## Step 3: Set Your Pipeline Parameters\n",
        "\n",
        "**üìù TODO: Update these paths with your actual data!**\n",
        "\n",
        "**Important**: Run this pipeline twice - once for your IP sample and once for your control/input sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cfcdfd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Update these paths to match your data location\n",
        "fastq1 = \"/path/to/your/sample_R1.fq.gz\"      # Single-end reads\n",
        "basename = \"my_chip_sample\"                    # Sample name (e.g., \"IP_sample\" or \"control\")\n",
        "output_dir = \"/path/to/output_directory\"       # Where to save results\n",
        "\n",
        "# Reference genome files (ask your instructor for these paths)\n",
        "genome_index = \"/path/to/bowtie2_index\"        # Bowtie2 genome index\n",
        "\n",
        "# Analysis settings\n",
        "threads = 2                                     # Number of CPU threads\n",
        "# TODO: Update this to your accessible directory for Singularity\n",
        "BIND_DIR = \"/path/to/accessible_directory/\"    # Directory accessible to Singularity\n",
        "\n",
        "print(f\"Sample name: {basename}\")\n",
        "print(f\"Sample type: {fastq1}\")\n",
        "print(f\"Threads: {threads}\")\n",
        "print(\"‚ö† Remember to update the file paths above before running!\")\n",
        "print(\"\\nüí° Tip: Process both IP and control samples using this notebook!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee3e62b",
      "metadata": {},
      "source": [
        "## Step 4: Create Output Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d2ad43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory structure\n",
        "OUTPUT_DIR = os.path.join(os.path.abspath(output_dir), f\"{basename}_results\")\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create subdirectories\n",
        "qc_dir = os.path.join(OUTPUT_DIR, f\"{basename}_qc\")\n",
        "picard_dir = os.path.join(OUTPUT_DIR, f\"{basename}_picard\")\n",
        "\n",
        "for d in [qc_dir, picard_dir]:\n",
        "    Path(d).mkdir(exist_ok=True)\n",
        "\n",
        "# Initialize log file\n",
        "log_file = os.path.join(OUTPUT_DIR, f\"{basename}_pipeline.log\")\n",
        "\n",
        "print(f\"‚úì Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"‚úì QC directory: {qc_dir}\")\n",
        "print(f\"‚úì Picard directory: {picard_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7880052",
      "metadata": {},
      "source": [
        "## Step 5: Trim Adapters and Run Quality Control\n",
        "\n",
        "**What does this do?** Removes adapter sequences and low-quality bases, then runs FastQC on trimmed reads.\n",
        "\n",
        "**üìù TODO: Run Trim Galore with integrated FastQC.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bcc3567",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: Trimming and Quality Control\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nRunning Trim Galore (with FastQC)...\")\n",
        "\n",
        "# TODO: Complete the Trim Galore command\n",
        "# Hint: You need to specify the correct container and complete the command parameters\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{???}\",  # TODO: Which container should you use for trim_galore?\n",
        "    \"trim_galore\",\n",
        "    # TODO: Add the following parameters:\n",
        "    # - Number of threads/cores to use\n",
        "    # - Output format (gzip)\n",
        "    # - Base name for output files\n",
        "    # - FastQC arguments with output directory\n",
        "    # - Output directory\n",
        "    # - Input FASTQ file\n",
        "]\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Trimming and QC completed!\")\n",
        "    fastq1_trimmed = os.path.join(OUTPUT_DIR, f\"{basename}_trimmed.fq.gz\")\n",
        "    print(f\"  Trimmed reads: {fastq1_trimmed}\")\n",
        "    print(f\"  QC reports: {qc_dir}/\")\n",
        "    \n",
        "    # Show trimming stats from output\n",
        "    if \"Total reads processed:\" in result.stdout:\n",
        "        for line in result.stdout.split('\\n'):\n",
        "            if 'reads processed' in line or 'reads with adapters' in line or 'Quality-trimmed' in line:\n",
        "                print(f\"  {line.strip()}\")\n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìù QUESTIONS TO ANSWER:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. What percentage of your reads contained adapters?\")\n",
        "print(\"2. Why is it important to remove adapters before alignment?\")\n",
        "print(\"3. What is the average read length after trimming?\")\n",
        "print(\"4. Check the FastQC report - what is the per-base sequence quality?\")\n",
        "print(\"5. Are there any overrepresented sequences? What could they be?\")\n",
        "print(\"\\nüí° Tip: Open the FastQC HTML report in a browser to visualize quality metrics\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8b440d",
      "metadata": {},
      "source": [
        "## Step 6: Align Reads with Bowtie2\n",
        "\n",
        "**What does this do?** Maps single-end reads to the reference genome.\n",
        "\n",
        "**üìù TODO: Complete the Bowtie2 alignment command.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b283423",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 2: Alignment with Bowtie2\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fastq1_trimmed = os.path.join(OUTPUT_DIR, f\"{basename}_trimmed.fq.gz\")\n",
        "sam_file = os.path.join(OUTPUT_DIR, f\"{basename}.sam\")\n",
        "\n",
        "print(\"\\nAligning reads to genome...\")\n",
        "\n",
        "# TODO: Complete the Bowtie2 command for single-end reads\n",
        "# Hint: Check which container has bowtie2 installed\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{???}\",  # TODO: Which container should you use for bowtie2?\n",
        "    \"bowtie2\",\n",
        "    # TODO: Add the following parameters:\n",
        "    # - Alignment sensitivity mode (hint: --very-sensitive)\n",
        "    # - Number of threads\n",
        "    # - Index basename (variable: genome_index)\n",
        "    # - Input reads (variable: fastq1_trimmed, use -U for single-end)\n",
        "]\n",
        "\n",
        "print(\"Running Bowtie2 (this may take several minutes)...\")\n",
        "with open(sam_file, 'w') as outfile:\n",
        "    result = subprocess.run(cmd, stdout=outfile, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Alignment completed!\")\n",
        "    print(f\"  SAM file: {sam_file}\")\n",
        "    \n",
        "    # Parse alignment stats\n",
        "    print(\"\\n  Alignment Statistics:\")\n",
        "    for line in result.stderr.split('\\n'):\n",
        "        if 'reads; of these:' in line or 'aligned concordantly' in line or 'aligned exactly' in line or 'overall alignment rate' in line:\n",
        "            print(f\"  {line.strip()}\")\n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìù QUESTIONS TO ANSWER:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. What is your overall alignment rate? Is it acceptable (typically >70%)?\")\n",
        "print(\"2. If the alignment rate is low (<70%), what could be the reasons?\")\n",
        "print(\"   - Wrong reference genome?\")\n",
        "print(\"   - High adapter contamination?\")\n",
        "print(\"   - Poor quality reads?\")\n",
        "print(\"   - Sample contamination?\")\n",
        "print(\"3. How many reads aligned exactly once vs. multiple times?\")\n",
        "print(\"4. Why might some reads align to multiple locations?\")\n",
        "print(\"5. Compare IP vs Control alignment rates - are they similar?\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d63e638",
      "metadata": {},
      "source": [
        "## Step 7: Convert SAM to BAM and Sort\n",
        "\n",
        "**What does this do?** Converts to binary format (BAM) and sorts by genomic coordinates.\n",
        "\n",
        "**üìù TODO: Complete the conversion and sorting steps.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9ca290",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 3: Convert SAM to BAM and Sort\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sam_file = os.path.join(OUTPUT_DIR, f\"{basename}.sam\")\n",
        "sorted_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted.bam\")\n",
        "\n",
        "print(\"\\n[1/2] Converting SAM to BAM and sorting...\")\n",
        "\n",
        "# TODO: Complete the samtools sort command\n",
        "# Hint: Which container has samtools installed?\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{???}\",  # TODO: Which container should you use for samtools?\n",
        "    \"samtools\", \"sort\",\n",
        "    # TODO: Add the following parameters:\n",
        "    # - Number of threads (hint: use -@)\n",
        "    # - Output file (hint: use -o with sorted_bam variable)\n",
        "    # - Input file (variable: sam_file)\n",
        "]\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Sorting completed!\")\n",
        "    print(f\"  Sorted BAM: {sorted_bam}\")\n",
        "    \n",
        "    # Get file size\n",
        "    bam_size = os.path.getsize(sorted_bam) / (1024**3)  # Convert to GB\n",
        "    print(f\"  File size: {bam_size:.2f} GB\")\n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")\n",
        "\n",
        "print(\"\\n[2/2] Fixing BAM header (removing Bowtie2 version info)...\")\n",
        "# This step is needed for compatibility with Picard\n",
        "header_file = os.path.join(OUTPUT_DIR, f\"{basename}_header.txt\")\n",
        "header_filtered = os.path.join(OUTPUT_DIR, f\"{basename}_header_filtered.txt\")\n",
        "\n",
        "# Extract header\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{samtools_container}\",\n",
        "    \"samtools\", \"view\", \"-H\", sorted_bam\n",
        "]\n",
        "result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "with open(header_file, 'w') as f:\n",
        "    f.write(result.stdout)\n",
        "\n",
        "# Filter @PG lines\n",
        "with open(header_file, 'r') as f:\n",
        "    with open(header_filtered, 'w') as out:\n",
        "        for line in f:\n",
        "            if not line.startswith('@PG'):\n",
        "                out.write(line)\n",
        "\n",
        "# Reheader\n",
        "reheaded_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted_reheaded.bam\")\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{trimgalore_container}\",\n",
        "    \"samtools\", \"reheader\",\n",
        "    \"-P\", header_filtered,\n",
        "    sorted_bam\n",
        "]\n",
        "with open(reheaded_bam, 'w') as outfile:\n",
        "    subprocess.run(cmd, stdout=outfile, capture_output=False)\n",
        "\n",
        "# Replace original\n",
        "os.replace(reheaded_bam, sorted_bam)\n",
        "os.remove(header_file)\n",
        "os.remove(header_filtered)\n",
        "os.remove(sam_file)\n",
        "\n",
        "print(\"‚úì Header fixed and SAM file removed!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìù QUESTIONS TO ANSWER:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. What is the file size of your sorted BAM file?\")\n",
        "print(\"2. Why is BAM format better than SAM for storage?\")\n",
        "print(\"3. Why is coordinate sorting necessary for downstream analysis?\")\n",
        "print(\"4. What is the purpose of removing the SAM file after conversion?\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba01e679",
      "metadata": {},
      "source": [
        "## Step 8: Mark Duplicates with Picard\n",
        "\n",
        "**What does this do?** Identifies PCR duplicate reads (same genomic position).\n",
        "\n",
        "**Why mark duplicates in ChIP-seq?** PCR duplicates can artificially inflate signal and should be marked (and often removed) to avoid false positive peaks.\n",
        "\n",
        "**üìù TODO: Run Picard MarkDuplicates.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a9b5a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 4: Mark duplicate reads\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sorted_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted.bam\")\n",
        "mkdup_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted_mkdup.bam\")\n",
        "dup_metrics = os.path.join(picard_dir, f\"{basename}_dup.txt\")\n",
        "\n",
        "print(\"\\nMarking duplicates with Picard...\")\n",
        "\n",
        "# TODO: Complete the Picard MarkDuplicates command\n",
        "# Hint: Picard has its own container separate from samtools\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{???}\",  # TODO: Which container should you use for picard?\n",
        "    \"picard\", \"MarkDuplicates\",\n",
        "    # TODO: Add the following parameters:\n",
        "    # - Input BAM file (hint: -I with sorted_bam variable)\n",
        "    # - Output BAM file (hint: -O with mkdup_bam variable)\n",
        "    # - Metrics file (hint: -M with dup_metrics variable)\n",
        "    # - REMOVE_DUPLICATES setting (should be \"false\" to mark but not remove)\n",
        "    # - ASSUME_SORT_ORDER setting (should be \"coordinate\")\n",
        "]\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Duplicate marking completed!\")\n",
        "    print(f\"  Marked BAM: {mkdup_bam}\")\n",
        "    print(f\"  Metrics file: {dup_metrics}\")\n",
        "    \n",
        "    # Parse duplicate metrics\n",
        "    print(\"\\n  üìä Duplicate Statistics:\")\n",
        "    with open(dup_metrics, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"LIBRARY\"):\n",
        "                header = line.strip().split('\\t')\n",
        "                data = next(f).strip().split('\\t')\n",
        "                # Find indices\n",
        "                for i, col in enumerate(header):\n",
        "                    if col == \"UNPAIRED_READS_EXAMINED\":\n",
        "                        unpaired_reads = int(data[i])\n",
        "                    elif col == \"UNPAIRED_READ_DUPLICATES\":\n",
        "                        unpaired_dups = int(data[i])\n",
        "                    elif col == \"PERCENT_DUPLICATION\":\n",
        "                        dup_rate = float(data[i])\n",
        "                \n",
        "                print(f\"    Total reads examined: {unpaired_reads:,}\")\n",
        "                print(f\"    Duplicate reads: {unpaired_dups:,}\")\n",
        "                print(f\"    Duplication rate: {dup_rate*100:.2f}%\")\n",
        "                \n",
        "                if dup_rate < 0.2:\n",
        "                    print(f\"    ‚úì Good library complexity (<20% duplicates)\")\n",
        "                elif dup_rate < 0.5:\n",
        "                    print(f\"    ‚ö† Moderate duplication (20-50%)\")\n",
        "                else:\n",
        "                    print(f\"    ‚úó High duplication (>50%) - may indicate low complexity\")\n",
        "                break\n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìù QUESTIONS TO ANSWER:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"1. What is your duplication rate? Record the exact percentage.\")\n",
        "print(\"2. What does the duplication rate tell you about library quality?\")\n",
        "print(\"   - <20%: Excellent library complexity\")\n",
        "print(\"   - 20-50%: Acceptable, but could be improved\")\n",
        "print(\"   - >50%: Poor library complexity (potential issues)\")\n",
        "print(\"3. What causes PCR duplicates in ChIP-seq experiments?\")\n",
        "print(\"4. Why do we mark duplicates instead of removing them immediately?\")\n",
        "print(\"5. How might high duplication affect peak calling results?\")\n",
        "print(\"6. Compare duplication rates between IP and control samples - are they similar?\")\n",
        "print(\"7. What could cause unusually high duplication rates?\")\n",
        "print(\"   - Low input DNA amount?\")\n",
        "print(\"   - Too many PCR cycles?\")\n",
        "print(\"   - Poor library complexity?\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a546c96f",
      "metadata": {},
      "source": [
        "## Step 9: Filter Low Quality Reads and Remove Duplicates\n",
        "\n",
        "**What does this do?** Removes:\n",
        "- Duplicate reads (FLAG 1024)\n",
        "- Low mapping quality reads (MAPQ < 20)\n",
        "\n",
        "**MAPQ threshold**: 20 means 99% probability the read is correctly mapped.\n",
        "\n",
        "**üìù TODO: Apply quality filters.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05892739",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 5: Filter low quality reads and remove duplicates\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "mkdup_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted_mkdup.bam\")\n",
        "filtered_bam = os.path.join(OUTPUT_DIR, f\"{basename}_sorted_mkdup_filtered.bam\")\n",
        "final_bam = os.path.join(OUTPUT_DIR, f\"{basename}_final.bam\")\n",
        "\n",
        "print(\"\\nFiltering reads with MAPQ < 20 and removing duplicates...\")\n",
        "\n",
        "# TODO: Complete the filtering command\n",
        "# Hint: -F 1024 removes duplicates, -q 20 keeps only high-quality alignments\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{samtools_container}\",\n",
        "    \"samtools\", \"view\",\n",
        "    \"-@\", str(threads),\n",
        "    # TODO: Add the following parameters:\n",
        "    # - Filter flag to remove duplicates (hint: -F 1024)\n",
        "    # - Minimum mapping quality (hint: -q 20)\n",
        "    # - Output format as BAM (hint: -b)\n",
        "    # - Output file (hint: -o with filtered_bam variable)\n",
        "    # - Input file (variable: mkdup_bam)\n",
        "]\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì Filtering completed!\")\n",
        "    \n",
        "    # Sort the filtered BAM\n",
        "    print(\"Sorting filtered BAM...\")\n",
        "    cmd = [\n",
        "        \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "        \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "        f\"docker://{samtools_container}\",\n",
        "        \"samtools\", \"sort\",\n",
        "        \"-@\", str(threads),\n",
        "        \"-o\", final_bam,\n",
        "        filtered_bam\n",
        "    ]\n",
        "    subprocess.run(cmd, capture_output=False)\n",
        "    \n",
        "    print(\"‚úì Final BAM created!\")\n",
        "    print(f\"  Final BAM: {final_bam}\")\n",
        "    \n",
        "    # Generate flagstats\n",
        "    flagstats_file = os.path.join(qc_dir, f\"{basename}_flagstats.txt\")\n",
        "    cmd = [\n",
        "        \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "        \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "        f\"docker://{samtools_container}\",\n",
        "        \"samtools\", \"flagstat\",\n",
        "        \"-@\", str(threads),\n",
        "        final_bam\n",
        "    ]\n",
        "    result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "    \n",
        "    with open(flagstats_file, 'w') as f:\n",
        "        f.write(result.stdout)\n",
        "    \n",
        "    print(f\"\\n  üìä Final Read Statistics:\")\n",
        "    for line in result.stdout.split('\\n'):\n",
        "        if 'mapped (' in line or 'properly paired' in line or 'singletons' in line:\n",
        "            print(f\"    {line.strip()}\")\n",
        "    \n",
        "    print(f\"\\n  Full stats saved to: {flagstats_file}\")\n",
        "    \n",
        "    # Clean up intermediate files\n",
        "    os.remove(mkdup_bam)\n",
        "    os.remove(filtered_bam)\n",
        "    print(\"\\n‚úì Intermediate files cleaned up\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìù QUESTIONS TO ANSWER:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"1. How many reads remained after filtering?\")\n",
        "    print(\"2. What percentage of reads were removed during filtering?\")\n",
        "    print(\"3. Why do we use MAPQ >= 20 as the quality threshold?\")\n",
        "    print(\"   (Hint: What does MAPQ=20 mean in terms of mapping confidence?)\")\n",
        "    print(\"4. Why is it important to remove low-quality alignments in ChIP-seq?\")\n",
        "    print(\"5. What is the difference between marked duplicates and removed duplicates?\")\n",
        "    print(\"6. Calculate: Starting reads ‚Üí After trimming ‚Üí After alignment ‚Üí After filtering\")\n",
        "    print(\"   What percentage of raw reads made it to the final BAM file?\")\n",
        "    print(\"7. Is your final read count sufficient for peak calling (typically need >5-10M)?\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88e66493",
      "metadata": {},
      "source": [
        "## Step 10: Create BigWig Coverage Track\n",
        "\n",
        "**What does this do?** Generates a normalized genome browser track for visualization.\n",
        "\n",
        "**Normalization**: RPKM (Reads Per Kilobase per Million mapped reads) makes samples comparable.\n",
        "\n",
        "**üìù TODO: Generate the coverage track.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323ae6b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"STEP 6: Generate BigWig coverage track\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "final_bam = os.path.join(OUTPUT_DIR, f\"{basename}_final.bam\")\n",
        "bigwig_file = os.path.join(OUTPUT_DIR, f\"{basename}_final_RPKM_Norm_bs10.bw\")\n",
        "\n",
        "print(\"\\n[1/2] Indexing BAM file...\")\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{samtools_container}\",\n",
        "    \"samtools\", \"index\",\n",
        "    \"-@\", str(threads),\n",
        "    final_bam\n",
        "]\n",
        "subprocess.run(cmd, capture_output=False)\n",
        "print(\"‚úì Indexing done\")\n",
        "\n",
        "print(\"\\n[2/2] Creating normalized BigWig track...\")\n",
        "\n",
        "# TODO: Complete the bamCoverage command\n",
        "# Hint: bamCoverage is part of deeptools\n",
        "cmd = [\n",
        "    \"singularity\", \"exec\", \"-e\", \"--no-home\",\n",
        "    \"--bind\", f\"{BIND_DIR}:{BIND_DIR}\",\n",
        "    f\"docker://{???}\",  # TODO: Which container should you use for bamCoverage?\n",
        "    \"bamCoverage\",\n",
        "    # TODO: Add the following parameters:\n",
        "    # - Input BAM file (hint: -b with final_bam variable)\n",
        "    # - Output BigWig file (hint: -o with bigwig_file variable)\n",
        "    # - Bin size (hint: -bs 10 for 10bp bins)\n",
        "    # - Normalization method (hint: --normalizeUsing RPKM)\n",
        "    # - Number of processors (hint: -p with threads variable)\n",
        "]\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úì BigWig file created!\")\n",
        "    print(f\"  File: {bigwig_file}\")\n",
        "    \n",
        "    # Get file size\n",
        "    bw_size = os.path.getsize(bigwig_file) / (1024**2)  # Convert to MB\n",
        "    print(f\"  File size: {bw_size:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\n  üí° Visualization:\")\n",
        "    print(f\"     - Load into IGV (Integrative Genomics Viewer)\")\n",
        "    print(f\"     - Load into UCSC Genome Browser\")\n",
        "    print(f\"     - Compare IP vs Control tracks to see enrichment\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìù QUESTIONS TO ANSWER:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"1. What is the file size of your BigWig file?\")\n",
        "    print(\"2. What does RPKM normalization do and why is it important?\")\n",
        "    print(\"3. What does the bin size (10bp) represent?\")\n",
        "    print(\"4. Why do we create BigWig files instead of using BAM for visualization?\")\n",
        "    print(\"5. After loading into IGV:\")\n",
        "    print(\"   - Do you see any regions with high signal?\")\n",
        "    print(\"   - Is the signal evenly distributed or localized?\")\n",
        "    print(\"   - How does the IP sample compare to the control?\")\n",
        "    print(\"6. What would you expect to see at true binding sites?\")\n",
        "    print(\"   (Hint: IP signal should be higher than control)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "else:\n",
        "    print(f\"‚úó Error: {result.stderr}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "046a6cbd",
      "metadata": {},
      "source": [
        "## Step 11: Pipeline Summary\n",
        "\n",
        "**Congratulations!** üéâ You've successfully processed your ChIP-seq sample!\n",
        "\n",
        "Let's review the results and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4da544d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"CHIP-SEQ SAMPLE PROCESSING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\n‚úì Sample processed: {basename}\")\n",
        "print(f\"‚úì Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"QUALITY METRICS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Read flagstats\n",
        "flagstats_file = os.path.join(qc_dir, f\"{basename}_flagstats.txt\")\n",
        "if os.path.exists(flagstats_file):\n",
        "    print(\"\\n1. Alignment Statistics:\")\n",
        "    with open(flagstats_file, 'r') as f:\n",
        "        content = f.read()\n",
        "        print(f\"   {content}\")\n",
        "\n",
        "# Read duplicate metrics\n",
        "dup_metrics = os.path.join(picard_dir, f\"{basename}_dup.txt\")\n",
        "if os.path.exists(dup_metrics):\n",
        "    print(\"\\n2. Duplication Rate:\")\n",
        "    with open(dup_metrics, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"LIBRARY\"):\n",
        "                header = line.strip().split('\\t')\n",
        "                data = next(f).strip().split('\\t')\n",
        "                for i, col in enumerate(header):\n",
        "                    if col == \"PERCENT_DUPLICATION\":\n",
        "                        dup_rate = float(data[i])\n",
        "                        print(f\"   {dup_rate*100:.2f}%\")\n",
        "                        if dup_rate < 0.2:\n",
        "                            print(f\"   ‚úì Good library quality\")\n",
        "                        elif dup_rate < 0.5:\n",
        "                            print(f\"   ‚ö† Moderate duplication\")\n",
        "                        else:\n",
        "                            print(f\"   ‚úó High duplication\")\n",
        "                break\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"OUTPUT FILES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìÅ Results Directory: {OUTPUT_DIR}\")\n",
        "print(f\"\\n  Key Files:\")\n",
        "print(f\"  ‚îú‚îÄ {basename}_final.bam              ‚Üê Final aligned reads (use for peak calling)\")\n",
        "print(f\"  ‚îú‚îÄ {basename}_final_RPKM_Norm_bs10.bw ‚Üê Coverage track (load in IGV)\")\n",
        "print(f\"  ‚îú‚îÄ {basename}_qc/\")\n",
        "print(f\"  ‚îÇ  ‚îú‚îÄ {basename}_flagstats.txt        ‚Üê Alignment statistics\")\n",
        "print(f\"  ‚îÇ  ‚îî‚îÄ FastQC reports                  ‚Üê Quality control reports\")\n",
        "print(f\"  ‚îî‚îÄ {basename}_picard/\")\n",
        "print(f\"     ‚îî‚îÄ {basename}_dup.txt              ‚Üê Duplicate metrics\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"NEXT STEPS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: Process BOTH samples!\")\n",
        "print(\"   1. Run this notebook for your IP sample\")\n",
        "print(\"   2. Run this notebook again for your control/input sample\")\n",
        "\n",
        "print(\"\\nüìä After processing both samples:\")\n",
        "print(\"   Use a separate peak calling script/notebook with:\")\n",
        "print(f\"   - IP BAM file: {basename}_final.bam\")\n",
        "print(\"   - Control BAM file: [control_name]_final.bam\")\n",
        "\n",
        "print(\"\\nüîç Peak Calling Tools:\")\n",
        "print(\"   - MACS2 (most common, good for transcription factors)\")\n",
        "print(\"   - MACS3 (updated version)\")\n",
        "print(\"   - HOMER (good for histone marks)\")\n",
        "print(\"   - SICER (good for broad domains)\")\n",
        "\n",
        "print(\"\\nüìà Downstream Analysis:\")\n",
        "print(\"   1. Peak annotation (assign peaks to genes)\")\n",
        "print(\"   2. Motif analysis (find DNA binding motifs)\")\n",
        "print(\"   3. Differential binding analysis (compare conditions)\")\n",
        "print(\"   4. Gene ontology enrichment\")\n",
        "print(\"   5. Integration with RNA-seq data\")\n",
        "\n",
        "print(\"\\nüí° Visualization Tips:\")\n",
        "print(\"   - Load BigWig files for both IP and control into IGV\")\n",
        "print(\"   - Look for regions where IP signal > control signal\")\n",
        "print(\"   - These regions indicate protein binding sites\")\n",
        "\n",
        "print(\"\\nüìù Review Questions:\")\n",
        "print(\"   1. What is your alignment rate? Is it acceptable?\")\n",
        "print(\"   2. What is your duplication rate? What does it mean?\")\n",
        "print(\"   3. How many reads remained after filtering?\")\n",
        "print(\"   4. What is the difference between IP and control samples?\")\n",
        "print(\"   5. Why do we need both IP and control for peak calling?\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Great job! Your ChIP-seq sample is ready for peak calling! üéâ\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
